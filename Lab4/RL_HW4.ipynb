{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaId-k8Enn3m"
   },
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0hHIC0nkTS8",
    "outputId": "b6d0f30c-066f-45b4-d4fb-f280dd71e42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo Value Function Using Optimal Policy:\n",
      "State (0, 1): -13.93\n",
      "State (4, 0): 37.77\n",
      "State (1, 2): -13.43\n",
      "State (3, 4): 85.24\n",
      "State (0, 4): -12.44\n",
      "State (4, 3): 100.00\n",
      "State (3, 1): 37.85\n",
      "State (2, 1): 28.18\n",
      "State (0, 2): -13.08\n",
      "State (1, 0): -60.34\n",
      "State (1, 3): -12.30\n",
      "State (4, 1): 73.94\n",
      "State (4, 4): 0.00\n",
      "State (0, 0): -53.90\n",
      "State (1, 1): -14.64\n",
      "State (0, 3): -12.50\n",
      "State (2, 0): 0.00\n",
      "State (4, 2): 89.00\n",
      "State (3, 0): 35.83\n",
      "State (1, 4): -11.92\n",
      "State (3, 3): 71.55\n",
      "\n",
      "Number of visited states: 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gridworld_lec4 import GridWorld\n",
    "from gw_lec4 import GridWorld5x5\n",
    "\n",
    "# Initialize the 5x5 GridWorld\n",
    "env = GridWorld5x5(p=0.5)\n",
    "\n",
    "# optimal policy frmo previous problem\n",
    "optimal_policy = {\n",
    "    (0, 0): 'D', (0, 1): 'D', (0, 2): 'D', (0, 3): 'L', (0, 4): 'L',\n",
    "    (1, 0): 'R', (1, 1): 'D', (1, 2): 'L', (1, 3): 'L', (1, 4): 'L',\n",
    "    (2, 1): 'D',\n",
    "    (3, 0): 'D', (3, 1): 'D', (3, 3): 'D', (3, 4): 'D',\n",
    "    (4, 0): 'R', (4, 1): 'R', (4, 2): 'R', (4, 3): 'R'\n",
    "}\n",
    "\n",
    "# MC parameters\n",
    "episodes = 10000  # epochs\n",
    "gamma = 0.9  # Discount factor\n",
    "value_function = {s: 0 for s in env.all_states()}  # value function\n",
    "returns = {s: [] for s in env.all_states()}  # state returns\n",
    "\n",
    "# MC w/ optimal policy\n",
    "for episode in range(episodes):\n",
    "    # Start from a random state\n",
    "    state = (np.random.randint(env.rows), np.random.randint(env.columns))\n",
    "    env.set_state(state)\n",
    "\n",
    "    # Generate episode following optimal policy\n",
    "    episode_sequence = []\n",
    "    while not env.game_over():\n",
    "        if state in optimal_policy:\n",
    "            action = optimal_policy[state]\n",
    "        else:\n",
    "            break  \n",
    "\n",
    "        next_state, reward = env.move(action)\n",
    "        episode_sequence.append((state, reward))\n",
    "        state = next_state\n",
    "\n",
    "    # Update value function\n",
    "    G = 0  \n",
    "    visited_states = set()\n",
    "    for state, reward in reversed(episode_sequence):\n",
    "        G = reward + gamma * G\n",
    "        if state not in visited_states:  # First-visit MC\n",
    "            visited_states.add(state)\n",
    "            returns[state].append(G)\n",
    "            value_function[state] = np.mean(returns[state])\n",
    "\n",
    "# State outputs\n",
    "visited_states_count = len([s for s in returns if returns[s]])\n",
    "print(\"Monte Carlo Value Function Using Optimal Policy:\")\n",
    "for state, value in value_function.items():\n",
    "    print(f\"State {state}: {value:.2f}\")\n",
    "print(\"\\nNumber of visited states:\", visited_states_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH4UlPWh1Xe0",
    "outputId": "522c90c6-e30c-462f-8b25-985cd7c2ee62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Optimal Policy:\n",
      "(0, 0): R, (0, 1): D, (0, 2): R, (0, 3): R, (0, 4): D, \n",
      "(1, 0): R, (1, 1): R, (1, 2): U, (1, 3): L, (1, 4): L, \n",
      "(2, 0): U, (2, 1): U, \n",
      "(3, 0): D, (3, 1): L, (3, 3): R, (3, 4): D, \n",
      "(4, 0): R, (4, 1): L, (4, 2): R, (4, 3): R, (4, 4): L, \n",
      "\n",
      "Number of Visited States: 21\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gridworld_lec4 import GridWorld\n",
    "from gw_lec4 import GridWorld5x5\n",
    "\n",
    "# Initialize the 5x5 GridWorld\n",
    "env = GridWorld5x5(p=0.5)\n",
    "\n",
    "# MC parameters\n",
    "episodes = 10000  # Epochs\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # epsilon-greedy\n",
    "\n",
    "# Initialize Q-value function and returns\n",
    "Q = {s: {a: 0 for a in env.actions(s)} for s in env.all_states()}\n",
    "returns = {s: {a: [] for a in env.actions(s)} for s in env.all_states()}\n",
    "\n",
    "# random policy\n",
    "policy = {s: np.random.choice(env.actions(s)) for s in env.all_states() if env.actions(s)}\n",
    "\n",
    "# keep track of visited states\n",
    "visited_states = set()\n",
    "\n",
    "# MC Exploring Starts Control\n",
    "for episode in range(episodes):\n",
    "    # Exploring start\n",
    "    state = (np.random.randint(env.rows), np.random.randint(env.columns))\n",
    "    env.set_state(state)\n",
    "    if not env.actions(state):\n",
    "        continue\n",
    "    action = np.random.choice(env.actions(state))\n",
    "\n",
    "    # Add the state to visited states\n",
    "    visited_states.add(state)\n",
    "\n",
    "    # Generate an episode following the current policy\n",
    "    episode_sequence = []\n",
    "    while not env.game_over():\n",
    "        next_state, reward = env.move(action)\n",
    "        episode_sequence.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if env.actions(state):\n",
    "            # Add the next state to visited states\n",
    "            visited_states.add(state)\n",
    "\n",
    "            # action base off epsilon greedy\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(env.actions(state))\n",
    "            else:\n",
    "                action = max(Q[state], key=Q[state].get)\n",
    "\n",
    "    # Calculate returns and update Q-values\n",
    "    G = 0  \n",
    "    visited_state_action_pairs = set()\n",
    "    for state, action, reward in reversed(episode_sequence):\n",
    "        G = reward + gamma * G\n",
    "        if (state, action) not in visited_state_action_pairs:  # First-visit MC\n",
    "            visited_state_action_pairs.add((state, action))\n",
    "            returns[state][action].append(G)\n",
    "            Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "            # Update the policy to be greedy with respect to Q\n",
    "            policy[state] = max(Q[state], key=Q[state].get)\n",
    "\n",
    "# Output the learned optimal policy\n",
    "print(\"Learned Optimal Policy:\")\n",
    "for r in range(env.rows):\n",
    "    for c in range(env.columns):\n",
    "        if (r, c) in policy:\n",
    "            print(f\"{(r, c)}: {policy[(r, c)]}\", end=\", \")\n",
    "    print()\n",
    "\n",
    "# Output the number of visited states\n",
    "print(\"\\nNumber of Visited States:\", len(visited_states))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAZur2Vv3jBO",
    "outputId": "57475c80-d659-4202-9f59-6276f7869237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-values:\n",
      "Q((0, 1), R): 21.47\n",
      "Q((0, 1), L): -6.01\n",
      "Q((0, 1), D): -4.28\n",
      "Q((4, 0), R): 54.26\n",
      "Q((4, 0), U): 49.59\n",
      "Q((1, 2), R): -8.18\n",
      "Q((1, 2), L): 12.29\n",
      "Q((1, 2), U): -8.07\n",
      "Q((3, 4), L): 78.47\n",
      "Q((3, 4), D): 100.00\n",
      "Q((0, 4), L): -8.65\n",
      "Q((0, 4), D): -9.97\n",
      "Q((4, 3), R): 100.00\n",
      "Q((4, 3), L): 70.66\n",
      "Q((4, 3), U): 90.20\n",
      "Q((3, 1), L): 37.83\n",
      "Q((3, 1), D): 55.75\n",
      "Q((3, 1), U): 24.14\n",
      "Q((2, 1), L): -100.00\n",
      "Q((2, 1), D): 43.72\n",
      "Q((2, 1), U): -8.56\n",
      "Q((0, 2), R): -7.20\n",
      "Q((0, 2), L): 14.46\n",
      "Q((0, 2), D): -7.05\n",
      "Q((1, 0), R): -7.41\n",
      "Q((1, 0), D): -100.00\n",
      "Q((1, 0), U): -9.00\n",
      "Q((1, 3), R): -9.83\n",
      "Q((1, 3), L): -10.95\n",
      "Q((1, 3), U): -8.27\n",
      "Q((4, 1), R): 78.28\n",
      "Q((4, 1), L): 53.72\n",
      "Q((4, 1), U): 48.03\n",
      "Q((4, 4), L): 0.00\n",
      "Q((4, 4), U): 0.00\n",
      "Q((0, 0), R): -5.41\n",
      "Q((0, 0), D): -11.30\n",
      "Q((1, 1), R): -6.49\n",
      "Q((1, 1), L): 13.31\n",
      "Q((1, 1), D): 15.93\n",
      "Q((1, 1), U): 16.35\n",
      "Q((0, 3), R): -9.67\n",
      "Q((0, 3), L): -7.98\n",
      "Q((0, 3), D): -9.50\n",
      "Q((2, 0), R): 0.00\n",
      "Q((2, 0), D): 0.00\n",
      "Q((2, 0), U): 0.00\n",
      "Q((4, 2), R): 88.37\n",
      "Q((4, 2), L): 55.99\n",
      "Q((3, 0), R): 43.41\n",
      "Q((3, 0), D): 48.14\n",
      "Q((3, 0), U): -100.00\n",
      "Q((1, 4), L): -9.02\n",
      "Q((1, 4), U): -12.00\n",
      "Q((3, 3), R): 88.00\n",
      "Q((3, 3), D): 88.32\n",
      "\n",
      "Number of Unique States Visited: 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gridworld_lec4 import GridWorld\n",
    "from gw_lec4 import GridWorld5x5\n",
    "\n",
    "# Initialize the 5x5 GridWorld \n",
    "env = GridWorld5x5(p=0.5)\n",
    "\n",
    "# Parameters\n",
    "episodes = 10000  # Number of episodes\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Epsilon for epsilon-soft policy\n",
    "\n",
    "# Initialize Q-value function and returns\n",
    "Q = {s: {a: 0 for a in env.actions(s)} for s in env.all_states()}\n",
    "returns = {s: {a: [] for a in env.actions(s)} for s in env.all_states()}\n",
    "\n",
    "# Initialize an epsilon-soft policy\n",
    "def epsilon_soft_policy(state):\n",
    "    actions = env.actions(state)\n",
    "    if not actions:\n",
    "        return None\n",
    "    best_action = max(Q[state], key=Q[state].get)\n",
    "    action_probs = {a: epsilon / len(actions) for a in actions}\n",
    "    action_probs[best_action] += 1 - epsilon\n",
    "    return np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
    "\n",
    "# Set to keep track of visited states\n",
    "visited_states = set()\n",
    "\n",
    "# On-policy First-visit MC Control\n",
    "for episode in range(episodes):\n",
    "    # Generate an episode following the epsilon-soft policy\n",
    "    state = (np.random.randint(env.rows), np.random.randint(env.columns))\n",
    "    env.set_state(state)\n",
    "    episode_sequence = []\n",
    "\n",
    "    while not env.game_over():\n",
    "        action = epsilon_soft_policy(state)\n",
    "        if action is None:\n",
    "            break\n",
    "        next_state, reward = env.move(action)\n",
    "        episode_sequence.append((state, action, reward))\n",
    "        visited_states.add(state)  # Track visited states\n",
    "        state = next_state\n",
    "\n",
    "    # Calculate returns and update Q-values\n",
    "    G = 0  # Return\n",
    "    visited_state_action_pairs = set()\n",
    "    for state, action, reward in reversed(episode_sequence):\n",
    "        G = reward + gamma * G\n",
    "        if (state, action) not in visited_state_action_pairs:  # First-visit MC\n",
    "            visited_state_action_pairs.add((state, action))\n",
    "            returns[state][action].append(G)\n",
    "            Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "            # Update the policy to be epsilon-soft\n",
    "            actions = env.actions(state)\n",
    "            if actions:\n",
    "                best_action = max(Q[state], key=Q[state].get)\n",
    "                for a in actions:\n",
    "                    if a == best_action:\n",
    "                        policy_prob = 1 - epsilon + (epsilon / len(actions))\n",
    "                    else:\n",
    "                        policy_prob = epsilon / len(actions)\n",
    "                    # No need to explicitly store policy probabilities, just use epsilon-soft policy\n",
    "\n",
    "# Output the learned Q-values\n",
    "print(\"Learned Q-values:\")\n",
    "for state in Q:\n",
    "    for action in Q[state]:\n",
    "        print(f\"Q({state}, {action}): {Q[state][action]:.2f}\")\n",
    "\n",
    "# Output the number of visited states\n",
    "print(\"\\nNumber of Unique States Visited:\", len(visited_states))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zUmYTB1m_xHP",
    "outputId": "54db8a42-3f9d-47b2-e8a9-6c967b5a13a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-values:\n",
      "Q((0, 1), R): 0.00\n",
      "Q((0, 1), L): 0.00\n",
      "Q((0, 1), D): 0.00\n",
      "Q((4, 0), R): 24.57\n",
      "Q((4, 0), U): 32.95\n",
      "Q((1, 2), R): 0.00\n",
      "Q((1, 2), L): 0.00\n",
      "Q((1, 2), U): 0.00\n",
      "Q((3, 4), L): 93.64\n",
      "Q((3, 4), D): 93.79\n",
      "Q((0, 4), L): 0.00\n",
      "Q((0, 4), D): 0.00\n",
      "Q((4, 3), R): 89.88\n",
      "Q((4, 3), L): 94.10\n",
      "Q((4, 3), U): 76.50\n",
      "Q((3, 1), L): 0.00\n",
      "Q((3, 1), D): 46.77\n",
      "Q((3, 1), U): 42.61\n",
      "Q((2, 1), L): -100.00\n",
      "Q((2, 1), D): 49.18\n",
      "Q((2, 1), U): -100.00\n",
      "Q((0, 2), R): 0.00\n",
      "Q((0, 2), L): 0.00\n",
      "Q((0, 2), D): 0.00\n",
      "Q((1, 0), R): 0.00\n",
      "Q((1, 0), D): -100.00\n",
      "Q((1, 0), U): -100.00\n",
      "Q((1, 3), R): 0.00\n",
      "Q((1, 3), L): 0.00\n",
      "Q((1, 3), U): 0.00\n",
      "Q((4, 1), R): 40.08\n",
      "Q((4, 1), L): 51.20\n",
      "Q((4, 1), U): 71.29\n",
      "Q((4, 4), L): 0.00\n",
      "Q((4, 4), U): 0.00\n",
      "Q((0, 0), R): 0.00\n",
      "Q((0, 0), D): 0.00\n",
      "Q((1, 1), R): 0.00\n",
      "Q((1, 1), L): 0.00\n",
      "Q((1, 1), D): 0.00\n",
      "Q((1, 1), U): 0.00\n",
      "Q((0, 3), R): 0.00\n",
      "Q((0, 3), L): 0.00\n",
      "Q((0, 3), D): 0.00\n",
      "Q((2, 0), R): 0.00\n",
      "Q((2, 0), D): 0.00\n",
      "Q((2, 0), U): 0.00\n",
      "Q((4, 2), R): 84.03\n",
      "Q((4, 2), L): 77.68\n",
      "Q((3, 0), R): 28.35\n",
      "Q((3, 0), D): -79.54\n",
      "Q((3, 0), U): -51.32\n",
      "Q((1, 4), L): 0.00\n",
      "Q((1, 4), U): 0.00\n",
      "Q((3, 3), R): 84.13\n",
      "Q((3, 3), D): 85.49\n",
      "\n",
      "Learned Target Policy:\n",
      "State (0, 1): R\n",
      "State (4, 0): U\n",
      "State (1, 2): U\n",
      "State (3, 4): D\n",
      "State (0, 4): L\n",
      "State (4, 3): L\n",
      "State (3, 1): D\n",
      "State (2, 1): D\n",
      "State (0, 2): D\n",
      "State (1, 0): R\n",
      "State (1, 3): L\n",
      "State (4, 1): U\n",
      "State (4, 4): L\n",
      "State (0, 0): R\n",
      "State (1, 1): R\n",
      "State (0, 3): R\n",
      "State (2, 0): R\n",
      "State (4, 2): R\n",
      "State (3, 0): R\n",
      "State (1, 4): U\n",
      "State (3, 3): D\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gridworld_lec4 import GridWorld\n",
    "from gw_lec4 import GridWorld5x5\n",
    "\n",
    "# Initialize the 5x5 GridWorld\n",
    "env = GridWorld5x5(p=0.5)\n",
    "\n",
    "# Parameters\n",
    "episodes = 10000  # epochs\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Initialize Q-value function and cumulative weights for all state-action pairs\n",
    "Q = {s: {a: 0 for a in env.actions(s)} for s in env.all_states()}\n",
    "C = {s: {a: 0 for a in env.actions(s)} for s in env.all_states()}\n",
    "\n",
    "# Initialize target policy\n",
    "target_policy = {s: np.random.choice(env.actions(s)) for s in env.all_states() if env.actions(s)}\n",
    "\n",
    "# Define a behavior policy\n",
    "def behavior_policy(state):\n",
    "    actions = env.actions(state)\n",
    "    if not actions:\n",
    "        return None\n",
    "    return np.random.choice(actions)\n",
    "\n",
    "# Off-policy First-visit MC Control\n",
    "for episode in range(episodes):\n",
    "    state = (np.random.randint(env.rows), np.random.randint(env.columns))\n",
    "    env.set_state(state)\n",
    "    episode_sequence = []\n",
    "\n",
    "    while not env.game_over():\n",
    "        action = behavior_policy(state)\n",
    "        if action is None:\n",
    "            break\n",
    "        next_state, reward = env.move(action)\n",
    "        episode_sequence.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    # Calculate returns and update Q-values using importance sampling\n",
    "    G = 0  \n",
    "    W = 1  \n",
    "    for state, action, reward in reversed(episode_sequence):\n",
    "        G = reward + gamma * G\n",
    "        C[state][action] += W\n",
    "        Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "\n",
    "        # Update the target policy to be greedy\n",
    "        best_action = max(Q[state], key=Q[state].get)\n",
    "        target_policy[state] = best_action\n",
    "\n",
    "        if action != best_action:\n",
    "            break\n",
    "        W *= 1 / (1 / len(env.actions(state)))  # Adjust the weight\n",
    "\n",
    "# Output the learned Q-values and target policy\n",
    "print(\"Learned Q-values:\")\n",
    "for state in Q:\n",
    "    for action in Q[state]:\n",
    "        print(f\"Q({state}, {action}): {Q[state][action]:.2f}\")\n",
    "\n",
    "print(\"\\nLearned Target Policy:\")\n",
    "for state in target_policy:\n",
    "    print(f\"State {state}: {target_policy[state]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h6-uyaskbzJ0"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Discretize the state space (e.g., using 10 bins for each of the 4 state variables)\n",
    "num_bins = 10\n",
    "state_bins = [\n",
    "    np.linspace(-4.8, 4.8, num_bins),  # Cart position\n",
    "    np.linspace(-4, 4, num_bins),      # Cart velocity\n",
    "    np.linspace(-0.418, 0.418, num_bins),  # Pole angle\n",
    "    np.linspace(-4, 4, num_bins)       # Pole angular velocity\n",
    "]\n",
    "\n",
    "# Initialize Q-values and returns\n",
    "Q = {}\n",
    "returns = {}\n",
    "\n",
    "# Initialize policy (randomly choose actions)\n",
    "policy = {}\n",
    "\n",
    "# Function to discretize continuous states\n",
    "def discretize_state(state):\n",
    "    binned_state = []\n",
    "    for i in range(len(state)):\n",
    "        binned_state.append(np.digitize(state[i], state_bins[i]))\n",
    "    return tuple(binned_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Y6vGfSFuhJnk"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Epsilon for epsilon-greedy policy\n",
    "episodes = 100  # Number of episodes\n",
    "\n",
    "# Function to choose an action using epsilon-greedy policy\n",
    "def epsilon_greedy_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Random action\n",
    "    else:\n",
    "        return np.argmax(Q.get(state, [0, 0]))  # Greedy action\n",
    "\n",
    "# Monte Carlo control loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]  # Reset environment and get the initial state\n",
    "    state = discretize_state(state)\n",
    "    episode_sequence = []\n",
    "\n",
    "    # Generate an episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state)\n",
    "        episode_sequence.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    # Calculate returns and update Q-values\n",
    "    G = 0  # Return\n",
    "    visited_state_action_pairs = set()\n",
    "    for state, action, reward in reversed(episode_sequence):\n",
    "        G = reward + gamma * G\n",
    "        if (state, action) not in visited_state_action_pairs:  # First-visit MC\n",
    "            visited_state_action_pairs.add((state, action))\n",
    "            if state not in returns:\n",
    "                returns[state] = {action: []}\n",
    "            if action not in returns[state]:\n",
    "                returns[state][action] = []\n",
    "            returns[state][action].append(G)\n",
    "\n",
    "            # Update Q-value\n",
    "            Q[state] = Q.get(state, [0, 0])\n",
    "            Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "            # Update policy\n",
    "            policy[state] = np.argmax(Q[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4pELa9phM_O",
    "outputId": "aadd6400-33e6-4415-d84d-12b21e3413d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 108.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atkha\\anaconda3\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:250: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "state = env.reset()[0]\n",
    "state = discretize_state(state)\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()  # Render the environment\n",
    "    action = policy.get(state, env.action_space.sample())  # Use learned policy\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = discretize_state(next_state)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total Reward:\", total_reward)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2IVlViDV3Wmt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the CarRacing-v0 environment\n",
    "env = gym.make(\"CarRacing-v3\", render_mode=\"human\")\n",
    "\n",
    "# Initialize pygame and set up the display\n",
    "pygame.init()\n",
    "window = pygame.display.set_mode((400, 300))\n",
    "pygame.display.set_caption(\"CarRacing Keyboard Control\")\n",
    "\n",
    "# Action: [steering, gas, brake]\n",
    "action = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "# Function to handle keyboard events\n",
    "def handle_keyboard_events():\n",
    "    global action\n",
    "    keys = pygame.key.get_pressed()\n",
    "    action = np.array([0.0, 0.0, 0.0])  # Reset action\n",
    "\n",
    "    if keys[pygame.K_LEFT]:\n",
    "        action[0] = -1.0  # Full left steering\n",
    "    if keys[pygame.K_RIGHT]:\n",
    "        action[0] = 1.0  # Full right steering\n",
    "    if keys[pygame.K_UP]:\n",
    "        action[1] = 1.0  # Full gas\n",
    "    if keys[pygame.K_DOWN]:\n",
    "        action[2] = 1.0  # Full brake\n",
    "\n",
    "# Main loop\n",
    "done = False\n",
    "state = env.reset()\n",
    "\n",
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            done = True\n",
    "\n",
    "    handle_keyboard_events()  # Update action based on keyboard input\n",
    "\n",
    "    # Take a step in the environment\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # Refresh the pygame display\n",
    "    pygame.display.flip()\n",
    "\n",
    "env.close()\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
