# -*- coding: utf-8 -*-
"""Copy of aly_khater.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mmt1hJpnUR9Zmajrs_oGX1_bHRL4gxx7
"""

'''
Aly Khater

Utilizing Qlearning to play Connect3

Policy Control Algorithm

Uses epsilon greedy for exploration and exploitation.

1. play()
  Player chooses an action based on epsilon greedy strategy
  Agent explores by selecting a random valid move, with center columns being prioritized
    for the agent to control the board more effectively.
  Agent chooses the move with the highest Q-value, prioritizing center moves
  Utilizes epsilon decay so the player moves towards exploitation

2. update()
  Agent updates Q-values.
  Agent traverses game states backwards.
  Rewards
    +7 total for winning. Incremental to reinfornce winning through looking backwards.
    -7 total for losing
    0 for tie
    +2 for blocking opponent's winning moove
    +3 for creating a double threat
    +0.5 for setting up a winning move

  Rewards are modified based off strategies, with the main reward coming from
  offensive strategies such as creating a double threat. Defensive moves like
  blocking are also rewarded so the agent can have a balance between offense and
  defence.

3. create_double_threat()
  Checks if placing piece creates double threat (two winning positions).

4. has_opponent_winning_move()
  Checks if the opponent has a winning move.

5. did_block_opponent()
  Checks if you can block the opponents winning move.

6. is_winning_move()
  Checks if move is a winning move

7. is_setup_move()
  Checks if move sets up a winning move.

8. check_three_in_a_row()
  Checks if there are three pieces in a row. Used in create_double_threat()


'''

from player import Player
import numpy as np
import pickle
from collections import defaultdict
import random


class AlyKhater(Player):
    def __init__(self, name, debug=False):
        super().__init__()
        self.set_name(name)
        self.Q = defaultdict(lambda: defaultdict(float))  # Q-table for state-action values
        self.alpha = 0.1  # 0.1 learning rate. Tested between 0.1-0.7
        self.gamma = 0.99  # 0.99 discount. Tested between .8 and .99
        self.epsilon = 1.0  # 1.0 Initial exploration rate. Tested between 0.8-1
        self.epsilon_decay = 0.999  # 0.999 Decay rate. Tested between .999-.9999
        self.epsilon_min = 0.03  # Min exploration. Tested between .01-.1
        self.debug = debug
        self.game_memory = []  # To store moves during the game

    def play(self):
        '''
        Selects a move from a state.
        '''
        # Determine the current state if current or other player
        state = -self.board.hash() if self.piece == -1 else self.board.hash()
        valid_moves = self.board.valid_moves()

        # Define center columns of board for optimal play
        center_columns = [1, 2, 3]
        center_moves = [move for move in valid_moves if move in center_columns]

        # Epsilon-greedy strategy
        if np.random.rand() < self.epsilon:  # Exploration
            # Chooses random move from center columns, otherwise chooses from rest of valid moves
            if center_moves:
                action = random.choice(center_moves)
            else:
                action = random.choice(valid_moves)
        else:  # Exploitation
            # Calculate Q-values for all valid moves
            q_values = [self.Q[state][a] for a in valid_moves]
            max_q = max(q_values)
            # Find all moves with the highest Q-value
            best_moves = [a for a in valid_moves if self.Q[state][a] == max_q]
            # Chooses a move from the center if one of the best moves is in the center
            center_best_moves = [move for move in best_moves if move in center_columns]
            if center_best_moves:
                action = random.choice(center_best_moves)
            else:
                action = random.choice(best_moves)

        # Store the move in memory for updates later
        self.game_memory.append((state, action))

        if self.debug:
            print(f'Player {self.name}: state={state}, action={action}')
        return action


    def update(self):
        '''
        Updates the policy. self.board.winner == 0 means we do update during
        the episode play. If self.board.winner != 0, we have are doing
        post-episode update.

        Returns
        -------
        None.

        '''
        if not self.board.game_over():
            return

        winner = self.board.winner
        reward = 5 if winner == self.piece else -5 if winner != 2 else 0  # Increased win and loss rewards

        # traversing the game_memory backward and update Q-values
        for state, action in reversed(self.game_memory):
            # Update Q-values with additional incentives
            if winner == self.piece:
                reward += 2.0  # Extra reward for a win
            elif winner == -self.piece:
                reward -= 2.0  # Extra penalty for a loss
            else:
                if self.is_setup_move(action):
                    reward += 0.5  # minimal reward for moves that setup a win
                if self.did_block_opponent(action, -self.piece):
                    reward += 2.0  # reward for blocking opponent's winning move
                if self.creates_double_threat(action):
                    reward += 3.0  # high reward for creating a double threat

            # Q-Learning
            next_valid_moves = self.board.valid_moves()
            max_future_q = max([self.Q[state][a] for a in next_valid_moves], default=0)
            old_q_value = self.Q[state][action]
            self.Q[state][action] += self.alpha * (reward + self.gamma * max_future_q - old_q_value)

            if self.debug:
                print(f"State: {state}, Action: {action}, Old Q-Value: {old_q_value}, "
                      f"New Q-Value: {self.Q[state][action]}, Reward: {reward}")

            # Set reward to 0 for non-terminal states
            reward = 0

        # Clear memory for the next game
        self.game_memory.clear()

        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def save(self):
        '''
        Saves in a pickle file.

        Returns
        -------
        None.

        '''
        with open(f"{self.name}.p", "wb") as file:
            pickle.dump(dict(self.Q), file)

    def load(self):
        '''
        Loads from a pickle file.

        Returns
        -------
        None.

        '''
        with open(f"{self.name}.p", "rb") as file:
            self.Q = defaultdict(lambda: defaultdict(float), pickle.load(file))




    def creates_double_threat(self, move):
        '''
        Checks if placing piece creates double threat (two winning positions).
        Parameters
        ----------
        move : int
            simulated move

        Returns
        -------
        bool
            true if the move creates a double-threat situation, false if not.
        '''
        # Simulate placing the piece for this move
        self.board.move(move, self.piece)

        # Check if there are exactly two moves on the next turn that would create a win
        winning_moves = 0
        for next_move in self.board.valid_moves():
            # Simulate the next move
            self.board.move(next_move, self.piece)

            # Check for three in a row
            if self.check_three_in_a_row():
                winning_moves += 1

            # Undo the simulated next move
            self.board.unmove()

            # If we find two winning moves, this is a double-threat move
            if winning_moves >= 2:
                self.board.unmove()  # Undo the original move
                return True

        # Undo the original move
        self.board.unmove()
        return False


    def has_opponent_winning_move(self, opponent_piece):
        '''
        Checks if the opponent has a winning move.
        Parameters
        ----------
        opponent_piece : int
            the opponent's piece

        Returns
        -------
        bool
            true if the opponent has a game-winning move, false if not
        '''
        valid_moves = self.board.valid_moves()
        for move in valid_moves:
            self.board.move(move, opponent_piece)  # make the move of the opponent piece
            if self.board.game_over() == opponent_piece:  # Check if this move results in a win
                self.board.unmove()  # Undo the move
                return True
            self.board.unmove()  # Undo the move
        return False

    def did_block_opponent(self, action, opponent_piece):
        '''
        Checks if you can block the opponents winning move.
        Parameters
        ----------
        action : int
            the action
        opponent_piece : int
            opponents piece

        Returns
        -------
        bool
            true if the action blocked a winning move, false if not
        '''
        self.board.move(action, self.piece)  # Temporarily make the move
        blocked = not self.has_opponent_winning_move(opponent_piece)  # Check if it blocked the winning move
        self.board.unmove()  # Undo the move
        return blocked

    def is_winning_move(self, action):
        '''
        Checks if move is a winning move

        Parameters
        ----------
        action : int
            the action
        Returns
        -------
        bool
            true if the action is a winning move, false if not.
        '''
        self.board.move(action, self.piece)  # Temporarily make the move
        winning_move = self.board.game_over() == self.piece  # Check if it wins the game
        self.board.unmove()  # Undo the move
        return winning_move

    def is_setup_move(self, action):
        '''
        Checks if move sets up a winning move.
        Parameters
        ----------
        action : int
            the action
        Returns
        -------
        bool
            true if the action sets up a winning move for next turn, false if not.
        '''
        self.board.move(action, self.piece)  # Temporarily make the move
        # Check if this move creates a winning move in the next turn
        setup_move = any(
            self.is_winning_move(move)
            for move in self.board.valid_moves()
        )
        self.board.unmove()  # Undo the move
        return setup_move


    def check_three_in_a_row(self):
        '''
        Checks if there are three pieces in a row. Used in create_double_threat()

        Parameters
        ----------
        None

        Returns
        -------
        bool
            true if there are exactly 3 pieces in a row, false if not.
        '''
        board_array = self.board.board

        # Check rows for 3 in a row
        for row in range(self.board.rows):
            for col in range(self.board.columns - 2):
                if (board_array[row, col] == self.piece and
                    board_array[row, col + 1] == self.piece and
                    board_array[row, col + 2] == self.piece):
                    return True

        # Check columns for 3 in a column
        for row in range(self.board.rows - 2):
            for col in range(self.board.columns):
                if (board_array[row, col] == self.piece and
                    board_array[row + 1, col] == self.piece and
                    board_array[row + 2, col] == self.piece):
                    return True

        # Check main diagonal for 3 in a row
        for row in range(self.board.rows - 2):
            for col in range(self.board.columns - 2):
                if (board_array[row, col] == self.piece and
                    board_array[row + 1, col + 1] == self.piece and
                    board_array[row + 2, col + 2] == self.piece):
                    return True

        # Check anti-diagonal for 3 in a row
        for row in range(2, self.board.rows):
            for col in range(self.board.columns - 2):
                if (board_array[row, col] == self.piece and
                    board_array[row - 1, col + 1] == self.piece and
                    board_array[row - 2, col + 2] == self.piece):
                    return True

        return False