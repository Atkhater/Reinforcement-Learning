{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Homework 3\n",
        "\n",
        "##Gridworld test"
      ],
      "metadata": {
        "id": "w76teK8tpy3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpbBxeoP4LXN"
      },
      "outputs": [],
      "source": [
        "from gridworld_hw3_q1 import GridWorld\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def GridWorld5x5():\n",
        "    rewards = {\n",
        "        (2,0): -100,\n",
        "        (4,4):  100\n",
        "    }\n",
        "\n",
        "    walls = [(2,2), (2,3), (2,4), (3,2)]\n",
        "\n",
        "    g = GridWorld(5, 5, start_position=(0, 0),\n",
        "            pass_through_reward=0, rewards=rewards, walls=walls)\n",
        "\n",
        "    return g\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    g = GridWorld5x5()\n",
        "\n",
        "    print(g.world)\n",
        "\n",
        "    while not g.game_over():\n",
        "        g.print()\n",
        "        print()\n",
        "\n",
        "        actions = g.actions()\n",
        "        print(actions)\n",
        "\n",
        "        c = \"\"\n",
        "        while c not in actions:\n",
        "            c = input()\n",
        "\n",
        "        s, r = g.move(c)\n",
        "\n",
        "        reward = g.world[s]\n",
        "\n",
        "        print(s, reward)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 1"
      ],
      "metadata": {
        "id": "e8ILOrVnpvQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#Policy Evaluation\n",
        "def policy_evaluation(policy, g, gamma=0.9, theta=1e-4, max_iterations=1000):\n",
        "    V = {s: 0 for s in g.all_states()}\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        iteration += 1\n",
        "        for s in g.all_states():\n",
        "            if s in g.rewards:\n",
        "                continue  # Skip terminal states\n",
        "\n",
        "            a = policy[s]  # Get action from policy\n",
        "            r, c = s\n",
        "            print(f\"Evaluating state {s} with action {a}\")\n",
        "\n",
        "            # Try moving\n",
        "            next_state, reward = g.move(a)\n",
        "            print(f\"Moved to {next_state}, reward: {reward}\")\n",
        "\n",
        "            # V(s_t+1) policy evaluation\n",
        "            v_new = reward + gamma * V[next_state]\n",
        "            delta = max(delta, abs(v_new - V[s]))\n",
        "            V[s] = v_new\n",
        "\n",
        "        print(f\"Iteration: {iteration}, Max Change: {delta}\")\n",
        "\n",
        "        # Convergence check or too many iterations\n",
        "        if delta < theta or iteration >= max_iterations:\n",
        "            print(f\"Converged after {iteration} iterations with max delta: {delta}\")\n",
        "            break\n",
        "    return V\n",
        "\n",
        "\n",
        "def policy_improvement(V, g, gamma=0.9):\n",
        "    policy = {}\n",
        "    for s in g.all_states():\n",
        "        if s in g.rewards:\n",
        "            continue  # Terminal states\n",
        "\n",
        "        best_value = float('-inf')\n",
        "        best_action = None\n",
        "\n",
        "        # Get possible actions for current state\n",
        "        possible_actions = g.actions(s)\n",
        "        print(f\"State {s}: Possible actions: {possible_actions}\")\n",
        "\n",
        "        for a in possible_actions:\n",
        "            next_state, reward = g.move(a)\n",
        "            value = reward + gamma * V[next_state]\n",
        "            print(f\"Action {a}: Moving to {next_state}, Value: {value}\")\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = a\n",
        "\n",
        "        policy[s] = best_action\n",
        "        print(f\"State {s}: Best action {policy[s]} with value {best_value}\")\n",
        "        #Test prints for blocks close to terminal state\n",
        "        print(f\"Value of (3, 3): {V.get((3, 3), 0)}\")\n",
        "        print(f\"Value of (4, 4): {V.get((4, 4), 0)}\")\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def policy_iteration(g, gamma=0.9):\n",
        "    # Initialize policy\n",
        "    policy = {\n",
        "        (0, 0): 'R', (0, 1): 'D', (0, 2): 'L', (0, 3): 'L', (0, 4): 'L',\n",
        "        (1, 0): 'R', (1, 1): 'D', (1, 2): 'L', (1, 3): 'L', (1, 4): 'L',\n",
        "        (2, 0): 'R', (2, 1): 'D',\n",
        "        (3, 0): 'D', (3, 1): 'D',              (3, 3): 'R', (3, 4): 'D',\n",
        "        (4, 0): 'R', (4, 1): 'R', (4, 2): 'R', (4, 3): 'R', (4, 4): 'D',\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        # Step 1: Policy Evaluation\n",
        "        print(f\"Initial position: {g.r, g.c}\")\n",
        "        V = policy_evaluation(policy, g, gamma)\n",
        "\n",
        "        # Step 2: Policy Improvement\n",
        "        new_policy = policy_improvement(V, g, gamma)\n",
        "\n",
        "        if new_policy == policy:\n",
        "            break  # Stop if policy is stable\n",
        "        policy = new_policy\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "def print_value_function(V, g):\n",
        "    # Grid size\n",
        "    rows, cols = g.rows, g.columns\n",
        "\n",
        "    # Iterate over rows and columns\n",
        "    for r in range(rows):\n",
        "        row_values = []\n",
        "        for c in range(cols):\n",
        "            # Get value of the current state (r, c)\n",
        "            value = V.get((r, c), 0)\n",
        "            #Cleaner decimal values\n",
        "            row_values.append(f\"{value:6.2f}\")\n",
        "        # Join the row and print it\n",
        "        print(\" | \".join(row_values))\n",
        "        print(\"-\" * (7 * cols))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    g = GridWorld5x5()\n",
        "\n",
        "    print(\"Initial grid layout (self.world):\")\n",
        "    print(g.world)\n",
        "\n",
        "    policy, V = policy_iteration(g)\n",
        "\n",
        "    print(\"Final policy:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            if (r, c) in policy:\n",
        "                print(f\"{(r, c)}: {policy[(r, c)]}\", end=\", \")\n",
        "        print()\n",
        "\n",
        "    print(\"Final value function:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            print(f\"{(r, c)}: {V.get((r, c), 0):.2f}\", end=\", \")\n",
        "        print()\n",
        "\n",
        "print(\"Final value function:\")\n",
        "print_value_function(V, g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xf43Duzj7ZvC",
        "outputId": "b3afc22f-cf71-4242-cc3c-4587b4912375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial grid layout (self.world):\n",
            "[[   0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.]\n",
            " [-100.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.  100.]]\n",
            "Initial position: (0, 0)\n",
            "Evaluating state (0, 0) with action R\n",
            "Moved to (0, 1), reward: 0.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (1, 1), reward: 0.0\n",
            "Evaluating state (0, 2) with action L\n",
            "Moved to (1, 0), reward: 0.0\n",
            "Evaluating state (0, 3) with action L\n",
            "Moved to (1, 0), reward: 0.0\n",
            "Evaluating state (0, 4) with action L\n",
            "Moved to (1, 0), reward: 0.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (1, 1), reward: 0.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (2, 1), reward: 0.0\n",
            "Evaluating state (1, 2) with action L\n",
            "Moved to (2, 0), reward: -100.0\n",
            "Evaluating state (1, 3) with action L\n",
            "Moved to (2, 0), reward: -100.0\n",
            "Evaluating state (1, 4) with action L\n",
            "Moved to (2, 0), reward: -100.0\n",
            "Evaluating state (2, 1) with action D\n",
            "Moved to (3, 0), reward: 0.0\n",
            "Evaluating state (3, 0) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 1) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 3) with action R\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (4, 0) with action R\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (4, 1) with action R\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (4, 2) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 3) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Iteration: 1, Max Change: 100.0\n",
            "Evaluating state (0, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 2) with action L\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 3) with action L\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (0, 4) with action L\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (1, 2) with action L\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (1, 3) with action L\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (1, 4) with action L\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (2, 1) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 0) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 1) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 3) with action R\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (4, 0) with action R\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (4, 1) with action R\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (4, 2) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 3) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Iteration: 2, Max Change: 100.0\n",
            "Evaluating state (0, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 2) with action L\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 3) with action L\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (0, 4) with action L\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (1, 2) with action L\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (1, 3) with action L\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (1, 4) with action L\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (2, 1) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 0) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 1) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 3) with action R\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (4, 0) with action R\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (4, 1) with action R\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (4, 2) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 3) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Iteration: 3, Max Change: 81.0\n",
            "Evaluating state (0, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 2) with action L\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 3) with action L\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (0, 4) with action L\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (1, 2) with action L\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (1, 3) with action L\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (1, 4) with action L\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (2, 1) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 0) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 1) with action D\n",
            "Moved to (4, 0), reward: 0.0\n",
            "Evaluating state (3, 3) with action R\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 1), reward: 0.0\n",
            "Evaluating state (4, 0) with action R\n",
            "Moved to (4, 2), reward: 0.0\n",
            "Evaluating state (4, 1) with action R\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (4, 2) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 3) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Iteration: 4, Max Change: 0\n",
            "Converged after 4 iterations with max delta: 0\n",
            "State (0, 0): Possible actions: ['D', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 0): Best action D with value 100.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 1): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 90.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 1): Best action D with value 100.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 2): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 90.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 2): Best action D with value 100.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 3): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 90.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 3): Best action D with value 100.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 4): Possible actions: ['D', 'L']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 90.0\n",
            "State (0, 4): Best action D with value 100.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 0): Possible actions: ['U', 'D', 'R']\n",
            "Action U: Moving to (3, 3), Value: 72.9\n",
            "Action D: Moving to (4, 3), Value: 90.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (1, 0): Best action R with value 100.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 1): Possible actions: ['U', 'D', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 72.9\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 90.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (1, 1): Best action D with value 100.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 2): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 72.9\n",
            "Action L: Moving to (3, 3), Value: 72.9\n",
            "Action R: Moving to (3, 4), Value: 72.9\n",
            "State (1, 2): Best action U with value 72.9\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 3): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 72.9\n",
            "Action L: Moving to (3, 3), Value: 72.9\n",
            "Action R: Moving to (3, 4), Value: 72.9\n",
            "State (1, 3): Best action U with value 72.9\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 4): Possible actions: ['U', 'L']\n",
            "Action U: Moving to (3, 4), Value: 72.9\n",
            "Action L: Moving to (3, 3), Value: 72.9\n",
            "State (1, 4): Best action U with value 72.9\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (2, 1): Possible actions: ['U', 'D', 'L']\n",
            "Action U: Moving to (3, 3), Value: 72.9\n",
            "Action D: Moving to (4, 3), Value: 90.0\n",
            "Action L: Moving to (4, 2), Value: 90.0\n",
            "State (2, 1): Best action D with value 90.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 0): Possible actions: ['U', 'D', 'R']\n",
            "Action U: Moving to (4, 2), Value: 90.0\n",
            "Action D: Moving to (4, 2), Value: 90.0\n",
            "Action R: Moving to (4, 3), Value: 90.0\n",
            "State (3, 0): Best action U with value 90.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 1): Possible actions: ['U', 'D', 'L']\n",
            "Action U: Moving to (3, 3), Value: 72.9\n",
            "Action D: Moving to (4, 3), Value: 90.0\n",
            "Action L: Moving to (4, 2), Value: 90.0\n",
            "State (3, 1): Best action D with value 90.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 3): Possible actions: ['D', 'R']\n",
            "Action D: Moving to (4, 2), Value: 90.0\n",
            "Action R: Moving to (4, 3), Value: 90.0\n",
            "State (3, 3): Best action D with value 90.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 4): Possible actions: ['D', 'L']\n",
            "Action D: Moving to (4, 3), Value: 90.0\n",
            "Action L: Moving to (4, 2), Value: 90.0\n",
            "State (3, 4): Best action D with value 90.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 0): Possible actions: ['U', 'R']\n",
            "Action U: Moving to (4, 2), Value: 90.0\n",
            "Action R: Moving to (4, 3), Value: 90.0\n",
            "State (4, 0): Best action U with value 90.0\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 1): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 3), Value: 72.9\n",
            "Action L: Moving to (3, 3), Value: 72.9\n",
            "Action R: Moving to (3, 4), Value: 72.9\n",
            "State (4, 1): Best action U with value 72.9\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 2): Possible actions: ['L', 'R']\n",
            "Action L: Moving to (3, 3), Value: 72.9\n",
            "Action R: Moving to (3, 4), Value: 72.9\n",
            "State (4, 2): Best action L with value 72.9\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 3): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 72.9\n",
            "Action L: Moving to (3, 3), Value: 72.9\n",
            "Action R: Moving to (3, 4), Value: 72.9\n",
            "State (4, 3): Best action U with value 72.9\n",
            "Value of (3, 3): 81.0\n",
            "Value of (4, 4): 0\n",
            "Initial position: (3, 4)\n",
            "Evaluating state (0, 0) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 2) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 2) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 3) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 4) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (2, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 2) with action L\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Evaluating state (4, 3) with action U\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Iteration: 1, Max Change: 100.0\n",
            "Evaluating state (0, 0) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 2) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 3) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 4) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 2) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 3) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 4) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (2, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 2) with action L\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Evaluating state (4, 3) with action U\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Iteration: 2, Max Change: 90.0\n",
            "Evaluating state (0, 0) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 2) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 3) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (0, 4) with action D\n",
            "Moved to (4, 3), reward: 0.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 2) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 3) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 4) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (2, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 2) with action L\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Evaluating state (4, 3) with action U\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Iteration: 3, Max Change: 0\n",
            "Converged after 3 iterations with max delta: 0\n",
            "State (0, 0): Possible actions: ['D', 'R']\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 0): Best action R with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 1): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 1): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 2): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 2): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 3): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 3): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 4): Possible actions: ['D', 'L']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "State (0, 4): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 0): Possible actions: ['U', 'D', 'R']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (1, 0): Best action R with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 1): Possible actions: ['U', 'D', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (1, 1): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 2): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (1, 2): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 3): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (1, 3): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 4): Possible actions: ['U', 'L']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "State (1, 4): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (2, 1): Possible actions: ['U', 'D', 'L']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action L: Moving to (4, 2), Value: 81.0\n",
            "State (2, 1): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 0): Possible actions: ['U', 'D', 'R']\n",
            "Action U: Moving to (4, 2), Value: 81.0\n",
            "Action D: Moving to (4, 2), Value: 81.0\n",
            "Action R: Moving to (4, 3), Value: 81.0\n",
            "State (3, 0): Best action U with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 1): Possible actions: ['U', 'D', 'L']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action L: Moving to (4, 2), Value: 81.0\n",
            "State (3, 1): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 3): Possible actions: ['D', 'R']\n",
            "Action D: Moving to (4, 2), Value: 81.0\n",
            "Action R: Moving to (4, 3), Value: 81.0\n",
            "State (3, 3): Best action D with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 4): Possible actions: ['D', 'L']\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action L: Moving to (4, 2), Value: 81.0\n",
            "State (3, 4): Best action D with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 0): Possible actions: ['U', 'R']\n",
            "Action U: Moving to (4, 2), Value: 81.0\n",
            "Action R: Moving to (4, 3), Value: 81.0\n",
            "State (4, 0): Best action U with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 1): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (4, 1): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 2): Possible actions: ['L', 'R']\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (4, 2): Best action L with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 3): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (4, 3): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "Initial position: (3, 4)\n",
            "Evaluating state (0, 0) with action R\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 2) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 2) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 3) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 4) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (2, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 2) with action L\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Evaluating state (4, 3) with action U\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Iteration: 1, Max Change: 100.0\n",
            "Evaluating state (0, 0) with action R\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 2) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 2) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 3) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 4) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (2, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 2) with action L\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Evaluating state (4, 3) with action U\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Iteration: 2, Max Change: 90.0\n",
            "Evaluating state (0, 0) with action R\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (0, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 2) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (0, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 0) with action R\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 1) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (1, 2) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 3) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (1, 4) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (2, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (3, 3) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (3, 4) with action D\n",
            "Moved to (4, 4), reward: 100.0\n",
            "Evaluating state (4, 0) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 1) with action U\n",
            "Moved to (3, 4), reward: 0.0\n",
            "Evaluating state (4, 2) with action L\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Evaluating state (4, 3) with action U\n",
            "Moved to (3, 3), reward: 0.0\n",
            "Iteration: 3, Max Change: 0\n",
            "Converged after 3 iterations with max delta: 0\n",
            "State (0, 0): Possible actions: ['D', 'R']\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 0): Best action R with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 1): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 1): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 2): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 2): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 3): Possible actions: ['D', 'L', 'R']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (0, 3): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (0, 4): Possible actions: ['D', 'L']\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "State (0, 4): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 0): Possible actions: ['U', 'D', 'R']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (1, 0): Best action R with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 1): Possible actions: ['U', 'D', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action D: Moving to (4, 4), Value: 100.0\n",
            "Action L: Moving to (4, 3), Value: 81.0\n",
            "Action R: Moving to (4, 4), Value: 100.0\n",
            "State (1, 1): Best action D with value 100.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 2): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (1, 2): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 3): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (1, 3): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (1, 4): Possible actions: ['U', 'L']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "State (1, 4): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (2, 1): Possible actions: ['U', 'D', 'L']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action L: Moving to (4, 2), Value: 81.0\n",
            "State (2, 1): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 0): Possible actions: ['U', 'D', 'R']\n",
            "Action U: Moving to (4, 2), Value: 81.0\n",
            "Action D: Moving to (4, 2), Value: 81.0\n",
            "Action R: Moving to (4, 3), Value: 81.0\n",
            "State (3, 0): Best action U with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 1): Possible actions: ['U', 'D', 'L']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action L: Moving to (4, 2), Value: 81.0\n",
            "State (3, 1): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 3): Possible actions: ['D', 'R']\n",
            "Action D: Moving to (4, 2), Value: 81.0\n",
            "Action R: Moving to (4, 3), Value: 81.0\n",
            "State (3, 3): Best action D with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (3, 4): Possible actions: ['D', 'L']\n",
            "Action D: Moving to (4, 3), Value: 81.0\n",
            "Action L: Moving to (4, 2), Value: 81.0\n",
            "State (3, 4): Best action D with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 0): Possible actions: ['U', 'R']\n",
            "Action U: Moving to (4, 2), Value: 81.0\n",
            "Action R: Moving to (4, 3), Value: 81.0\n",
            "State (4, 0): Best action U with value 81.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 1): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 3), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (4, 1): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 2): Possible actions: ['L', 'R']\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (4, 2): Best action L with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "State (4, 3): Possible actions: ['U', 'L', 'R']\n",
            "Action U: Moving to (3, 4), Value: 90.0\n",
            "Action L: Moving to (3, 3), Value: 90.0\n",
            "Action R: Moving to (3, 4), Value: 90.0\n",
            "State (4, 3): Best action U with value 90.0\n",
            "Value of (3, 3): 100.0\n",
            "Value of (4, 4): 0\n",
            "Final policy:\n",
            "(0, 0): R, (0, 1): D, (0, 2): D, (0, 3): D, (0, 4): D, \n",
            "(1, 0): R, (1, 1): D, (1, 2): U, (1, 3): U, (1, 4): U, \n",
            "(2, 1): U, \n",
            "(3, 0): U, (3, 1): U, (3, 3): D, (3, 4): D, \n",
            "(4, 0): U, (4, 1): U, (4, 2): L, (4, 3): U, \n",
            "Final value function:\n",
            "(0, 0): 90.00, (0, 1): 100.00, (0, 2): 100.00, (0, 3): 100.00, (0, 4): 100.00, \n",
            "(1, 0): 100.00, (1, 1): 100.00, (1, 2): 90.00, (1, 3): 90.00, (1, 4): 90.00, \n",
            "(2, 0): 0.00, (2, 1): 90.00, (2, 2): 0.00, (2, 3): 0.00, (2, 4): 0.00, \n",
            "(3, 0): 90.00, (3, 1): 90.00, (3, 2): 0.00, (3, 3): 100.00, (3, 4): 100.00, \n",
            "(4, 0): 90.00, (4, 1): 90.00, (4, 2): 90.00, (4, 3): 90.00, (4, 4): 0.00, \n",
            "Final value function:\n",
            " 90.00 | 100.00 | 100.00 | 100.00 | 100.00\n",
            "-----------------------------------\n",
            "100.00 | 100.00 |  90.00 |  90.00 |  90.00\n",
            "-----------------------------------\n",
            "  0.00 |  90.00 |   0.00 |   0.00 |   0.00\n",
            "-----------------------------------\n",
            " 90.00 |  90.00 |   0.00 | 100.00 | 100.00\n",
            "-----------------------------------\n",
            " 90.00 |  90.00 |  90.00 |  90.00 |   0.00\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 2"
      ],
      "metadata": {
        "id": "9xbvkKRuptIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gridworld_hw3_q1 import GridWorld\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "#From gw55_hw3\n",
        "def GridWorld5x5(p=0.9):\n",
        "    rewards = {\n",
        "        (2,0): -100,\n",
        "        (4,4):  100\n",
        "    }\n",
        "    walls = [(2,2), (2,3), (2,4), (3,2)]\n",
        "\n",
        "    # deterministic transition table\n",
        "    T = {\n",
        "        (0,0): { 'R': (0,1), 'D': (1,0) },\n",
        "        (0,1): { 'R': (0,2), 'L': (0,0), 'D': (1,1) },\n",
        "        (0,2): { 'R': (0,3), 'L': (0,1), 'D': (1,2) },\n",
        "        (0,3): { 'R': (0,4), 'L': (0,2), 'D': (1,3) },\n",
        "        (0,4): { 'L': (0,3), 'D': (1,4) },\n",
        "\n",
        "        (1,0): { 'R': (1,1), 'D': (2,0), 'U': (0,0) },\n",
        "        (1,1): { 'R': (1,2), 'L': (1,0), 'D': (2,1), 'U': (0,1) },\n",
        "        (1,2): { 'R': (1,3), 'L': (1,1), 'U': (0,2) },\n",
        "        (1,3): { 'R': (1,4), 'L': (1,2), 'U': (0,3) },\n",
        "        (1,4): { 'L': (1,3), 'U': (0,4) },\n",
        "\n",
        "        (2,0): { 'R': (2,1), 'D': (3,0), 'U': (1,0) },\n",
        "        (2,1): { 'L': (2,0), 'D': (3,1), 'U': (1,1) },\n",
        "\n",
        "        (3,0): { 'R': (3,1), 'D': (4,0), 'U': (2,0) },\n",
        "        (3,1): { 'L': (3,0), 'D': (4,1), 'U': (2,1) },\n",
        "        (3,3): { 'R': (3,4), 'D': (4,3) },\n",
        "        (3,4): { 'L': (3,3), 'D': (4,4) },\n",
        "\n",
        "        (4,0): { 'R': (4,1), 'U': (3,0) },\n",
        "        (4,1): { 'R': (4,2), 'L': (4,0), 'U': (3,1) },\n",
        "        (4,2): { 'R': (4,3), 'L': (4,1) },\n",
        "        (4,3): { 'R': (4,4), 'L': (4,2), 'U': (3,3) },\n",
        "        (4,4): { 'L': (4,3), 'U': (3,4) },\n",
        "    }\n",
        "\n",
        "    # convert deterministic transition table to stochastic one\n",
        "    # where we have probability 'p' of leaving the state and\n",
        "    # probability '1-p' to remain in the same state.\n",
        "\n",
        "    # this way it is easier to conver the table above to the\n",
        "    # stochastic version\n",
        "\n",
        "    for s in T:\n",
        "        ns_l = list(T[s].values())\n",
        "        for a in T[s]:\n",
        "            ns = T[s][a] #next state\n",
        "            rs = ns_l[np.random.choice(len(ns_l))] #random\n",
        "            if ns == rs:\n",
        "                T[s][a] = { ns: 1.0 }\n",
        "            else:\n",
        "                T[s][a] = { ns: p, rs: np.round(1-p,2) }\n",
        "\n",
        "    g = GridWorld(5, 5, start_position=(0, 0),\n",
        "            pass_through_reward=0, rewards=rewards, walls = walls)\n",
        "    g.probs = T\n",
        "\n",
        "    return g\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    g = GridWorld5x5(p=0.5)\n",
        "\n",
        "    policy = {\n",
        "        (0, 0): 'R', (0, 1): 'D', (0, 2): 'L', (0, 3): 'L', (0, 4): 'L',\n",
        "        (1, 0): 'R', (1, 1): 'D', (1, 2): 'L', (1, 3): 'L', (1, 4): 'L',\n",
        "        (2, 0): 'R', (2, 1): 'D',\n",
        "        (3, 0): 'D', (3, 1): 'D',              (3, 3): 'R', (3, 4): 'D',\n",
        "        (4, 0): 'R', (4, 1): 'R', (4, 2): 'R', (4, 3): 'R', (4, 4): 'D',\n",
        "    }\n",
        "\n",
        "    print(g.world)\n",
        "\n",
        "    while not g.game_over():\n",
        "        g.print()\n",
        "        print()\n",
        "\n",
        "        current_state = g.get_state()  # Get current state\n",
        "        actions = g.actions()  # Get available actions\n",
        "\n",
        "        # Choose action from policy in current state\n",
        "        if current_state in policy:\n",
        "            action = policy[current_state]\n",
        "        else:\n",
        "            # random action if no policy for current state\n",
        "            action = random.choice(actions)\n",
        "\n",
        "        print(f\"Chosen action: {action}\")\n",
        "\n",
        "        # Action moves, gets next state and reward\n",
        "        next_state, reward = g.move(action)\n",
        "        print(f\"Moved to state: {next_state}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "\n",
        "        # Set the agent's state to the new state\n",
        "        g.set_state(next_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pQ4NtkyMfOX",
        "outputId": "aa36eea7-9a2d-4b6d-a6ff-4b7f8d2772cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.]\n",
            " [-100.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.    0.]\n",
            " [   0.    0.    0.    0.  100.]]\n",
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (0, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (1, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: L\n",
            "Moved to state: (1, 2)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (1, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o| |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (0, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | |o| |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (1, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o| |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: L\n",
            "Moved to state: (1, 2)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: L\n",
            "Moved to state: (0, 2)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | |o| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: L\n",
            "Moved to state: (0, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (1, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (2, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B|o|x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (3, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| |o|x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (4, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (4, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "|o| | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (3, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "|o| |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (4, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "|o| | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 2)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 2)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o|G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 4)\n",
            "Reward: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 3"
      ],
      "metadata": {
        "id": "gt9_JnywpphY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_evaluation(policy, g, gamma=0.9, theta=1e-4, max_iterations=1000):\n",
        "    V = {s: 0 for s in g.all_states()}  # Initialize value function\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        iteration += 1\n",
        "        for s in g.all_states():\n",
        "            if s in g.rewards:  # Skip terminal states\n",
        "                continue\n",
        "\n",
        "            a = policy[s]  # Get the action from the current policy\n",
        "            next_states_probs = g.probs.get(s, {}).get(a, {})  # Get possible next states and their probabilities\n",
        "            v_new = 0\n",
        "            for next_state, prob in next_states_probs.items():\n",
        "                reward = g.world[next_state]  # Get the reward for the next state\n",
        "                v_new += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            delta = max(delta, abs(v_new - V[s]))\n",
        "            V[s] = v_new\n",
        "\n",
        "        if delta < theta or iteration >= max_iterations:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_improvement(V, g, gamma=0.9):\n",
        "    policy = {}\n",
        "    for s in g.all_states():\n",
        "        if s in g.rewards:  # Skip terminal states\n",
        "            continue\n",
        "\n",
        "        best_value = float('-inf')\n",
        "        best_action = None\n",
        "\n",
        "        possible_actions = g.actions(s)\n",
        "        for a in possible_actions:\n",
        "            next_states_probs = g.probs.get(s, {}).get(a, {})\n",
        "            value = 0\n",
        "            for next_state, prob in next_states_probs.items():\n",
        "                reward = g.world[next_state]\n",
        "                value += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = a\n",
        "\n",
        "        policy[s] = best_action\n",
        "    return policy\n",
        "\n",
        "def policy_iteration(g, gamma=0.9):\n",
        "    # Initialize a random policy\n",
        "    policy = {}\n",
        "    for s in g.all_states():\n",
        "        possible_actions = g.actions(s)\n",
        "        if possible_actions:\n",
        "            policy[s] = np.random.choice(possible_actions)  # Randomly assign an action to each state\n",
        "\n",
        "    while True:\n",
        "        # Step 1: Policy Evaluation\n",
        "        V = policy_evaluation(policy, g, gamma)\n",
        "\n",
        "        # Step 2: Policy Improvement\n",
        "        new_policy = policy_improvement(V, g, gamma)\n",
        "\n",
        "        if new_policy == policy:\n",
        "            break  # Stop if policy is stable\n",
        "        policy = new_policy\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "# Run policy iteration on the gridworld\n",
        "if __name__ == '__main__':\n",
        "    g = GridWorld5x5(p=0.5)\n",
        "\n",
        "    policy, V = policy_iteration(g)\n",
        "\n",
        "\n",
        "\n",
        "    while not g.game_over():\n",
        "        g.print()  # Print the gridworld state\n",
        "        print()\n",
        "\n",
        "        current_state = g.get_state()  # Get current state\n",
        "        actions = g.actions()  # Get available actions\n",
        "\n",
        "        # Choose action from policy for current state\n",
        "        if current_state in policy:\n",
        "            action = policy[current_state]\n",
        "        else:\n",
        "            # Random action if no policy for current shape\n",
        "            action = random.choice(actions)\n",
        "\n",
        "        print(f\"Chosen action: {action}\")\n",
        "\n",
        "        # Move action and gets next state and reward\n",
        "        next_state, reward = g.move(action)\n",
        "        print(f\"Moved to state: {next_state}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "\n",
        "        # Set the agent's state to the new state\n",
        "        g.set_state(next_state)\n",
        "\n",
        "    print(\"Optimal Policy:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            if (r, c) in policy:\n",
        "                print(f\"{(r, c)}: {policy[(r, c)]}\", end=\", \")\n",
        "        print()\n",
        "\n",
        "    print(\"Final Value Function:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            print(f\"{(r, c)}: {V.get((r, c), 0):.2f}\", end=\", \")\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGh8UZvFmwDR",
        "outputId": "63677d95-8260-4ea9-b2d8-77b4d8a38977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (0, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (1, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (2, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B|o|x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (3, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| |o|x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (2, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B|o|x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (3, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| |o|x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (4, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "|o| | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "|o| | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 2)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o|G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (3, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x|o| |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o|G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (3, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x|o| |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o|G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 4)\n",
            "Reward: 100.0\n",
            "Optimal Policy:\n",
            "(0, 0): R, (0, 1): D, (0, 2): D, (0, 3): L, (0, 4): D, \n",
            "(1, 0): U, (1, 1): D, (1, 2): L, (1, 3): L, (1, 4): L, \n",
            "(2, 1): D, \n",
            "(3, 0): D, (3, 1): D, (3, 3): R, (3, 4): D, \n",
            "(4, 0): R, (4, 1): R, (4, 2): R, (4, 3): R, \n",
            "Final Value Function:\n",
            "(0, 0): 22.63, (0, 1): 25.14, (0, 2): 25.00, (0, 3): 18.91, (0, 4): 17.02, \n",
            "(1, 0): 20.36, (1, 1): 30.87, (1, 2): 27.78, (1, 3): 21.01, (1, 4): 18.91, \n",
            "(2, 0): 0.00, (2, 1): 40.81, (2, 2): 0.00, (2, 3): 0.00, (2, 4): 0.00, \n",
            "(3, 0): 44.69, (3, 1): 45.35, (3, 2): 0.00, (3, 3): 84.64, (3, 4): 100.00, \n",
            "(4, 0): 53.96, (4, 1): 59.96, (4, 2): 79.28, (4, 3): 88.09, (4, 4): 0.00, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 4"
      ],
      "metadata": {
        "id": "AR8PSI_XpnKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(g, gamma=0.9, theta=1e-4, max_iterations=1000):\n",
        "    V = {s: 0 for s in g.all_states()}  # Initialize value function\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        iteration += 1\n",
        "        for s in g.all_states():\n",
        "            if s in g.rewards:  # Skip terminal states\n",
        "                continue\n",
        "\n",
        "            best_value = float('-inf')\n",
        "            possible_actions = g.actions(s)\n",
        "\n",
        "            # V(s) equation for each action\n",
        "            for a in possible_actions:\n",
        "                next_states_probs = g.probs.get(s, {}).get(a, {})\n",
        "                value = 0\n",
        "                for next_state, prob in next_states_probs.items():\n",
        "                    reward = g.world[next_state]\n",
        "                    value += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "                best_value = max(best_value, value)\n",
        "\n",
        "            delta = max(delta, abs(best_value - V[s]))\n",
        "            V[s] = best_value\n",
        "\n",
        "        if delta < theta or iteration >= max_iterations:\n",
        "            break\n",
        "\n",
        "    # Extract optimal policy based on value function\n",
        "    policy = {}\n",
        "    for s in g.all_states():\n",
        "        if s in g.rewards:  # Skip terminal states\n",
        "            continue\n",
        "\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "        possible_actions = g.actions(s)\n",
        "        for a in possible_actions:\n",
        "            next_states_probs = g.probs.get(s, {}).get(a, {})\n",
        "            value = 0\n",
        "            for next_state, prob in next_states_probs.items():\n",
        "                reward = g.world[next_state]\n",
        "                value += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = a\n",
        "\n",
        "        policy[s] = best_action\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "\n",
        "# Run value iteration on the gridworld\n",
        "if __name__ == '__main__':\n",
        "    g = GridWorld5x5(p=0.5)\n",
        "\n",
        "    policy, V = value_iteration(g)\n",
        "\n",
        "    while not g.game_over():\n",
        "        g.print()  # Print the gridworld state\n",
        "        print()\n",
        "\n",
        "        current_state = g.get_state()  # Get current state\n",
        "        actions = g.actions()  # Get available actions\n",
        "\n",
        "        # Choose action from policy for current state\n",
        "        if current_state in policy:\n",
        "            action = policy[current_state]\n",
        "        else:\n",
        "            # Random action if no policy defined for currnet state\n",
        "            action = random.choice(actions)\n",
        "\n",
        "        print(f\"Chosen action: {action}\")\n",
        "\n",
        "        # Action moves and gets next state and reward\n",
        "        next_state, reward = g.move(action)\n",
        "        print(f\"Moved to state: {next_state}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "\n",
        "        # Set the agent's state to the new state\n",
        "        g.set_state(next_state)\n",
        "\n",
        "    print(\"Optimal Policy:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            if (r, c) in policy:\n",
        "                print(f\"{(r, c)}: {policy[(r, c)]}\", end=\", \")\n",
        "        print()\n",
        "\n",
        "    print(\"Final Value Function:\")\n",
        "    for r in range(g.rows):\n",
        "        for c in range(g.columns):\n",
        "            print(f\"{(r, c)}: {V.get((r, c), 0):.2f}\", end=\", \")\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1AfovPtpXej",
        "outputId": "7534ece2-635e-47f9-c7a9-b3e1949ad32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (1, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (0, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (1, 0)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|o| | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: U\n",
            "Moved to state: (1, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (2, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B|o|x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (3, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| |o|x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: D\n",
            "Moved to state: (4, 1)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| |o| | |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 2)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | |o| |G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 3)\n",
            "Reward: 0.0\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "| | | | | |\n",
            "+-+-+-+-+-+\n",
            "|B| |x|x|x|\n",
            "+-+-+-+-+-+\n",
            "| | |x| | |\n",
            "+-+-+-+-+-+\n",
            "| | | |o|G|\n",
            "+-+-+-+-+-+\n",
            "\n",
            "Chosen action: R\n",
            "Moved to state: (4, 4)\n",
            "Reward: 100.0\n",
            "Optimal Policy:\n",
            "(0, 0): D, (0, 1): D, (0, 2): D, (0, 3): L, (0, 4): L, \n",
            "(1, 0): U, (1, 1): R, (1, 2): L, (1, 3): U, (1, 4): L, \n",
            "(2, 1): D, \n",
            "(3, 0): D, (3, 1): D, (3, 3): D, (3, 4): D, \n",
            "(4, 0): R, (4, 1): R, (4, 2): R, (4, 3): R, \n",
            "Final Value Function:\n",
            "(0, 0): 14.00, (0, 1): 15.56, (0, 2): 14.00, (0, 3): 10.59, (0, 4): 9.53, \n",
            "(1, 0): 15.56, (1, 1): 20.57, (1, 2): 15.56, (1, 3): 11.76, (1, 4): 9.58, \n",
            "(2, 0): 0.00, (2, 1): 30.15, (2, 2): 0.00, (2, 3): 0.00, (2, 4): 0.00, \n",
            "(3, 0): 30.15, (3, 1): 33.50, (3, 2): 0.00, (3, 3): 68.92, (3, 4): 81.02, \n",
            "(4, 0): 33.50, (4, 1): 37.22, (4, 2): 49.22, (4, 3): 72.15, (4, 4): 0.00, \n"
          ]
        }
      ]
    }
  ]
}